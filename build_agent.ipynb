{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "from scipy.interpolate import make_interp_spline\n",
    "\n",
    "# Classifier.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Conv2D,Flatten,MaxPooling2D\n",
    "from keras import optimizers\n",
    "from keras.constraints import maxnorm\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from dataset import DatasetCIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchAgent\n",
    "from batch_envs import LalEnvFirstAccuracy\n",
    "from batch_helpers import ReplayBuffer\n",
    "from batch_dqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start time.\n",
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier parameters.\n",
    "CLASSIFIER_NUMBER_OF_CLASSES = 10\n",
    "CLASSIFIER_NUMBER_OF_EPOCHS = 5\n",
    "CLASSIFIER_LEARNING_RATE = 0.01\n",
    "CLASSIFIER_BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for the agent.\n",
    "\n",
    "REPLAY_BUFFER_SIZE = 5e4\n",
    "PRIOROTIZED_REPLAY_EXPONENT = 3\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-3\n",
    "TARGET_COPY_FACTOR = 0.01\n",
    "BIAS_INITIALIZATION = 0\n",
    "\n",
    "NUMBER_OF_STATE_DATA = 1000\n",
    "TRAIN_DATASET_LENGTH = 1000\n",
    "\n",
    "# BatchAgent's parameters.\n",
    "\n",
    "DIRNAME = './batch_agent/' # The resulting batch_agent of this experiment will be written in a file.\n",
    "\n",
    "WARM_START_EPISODES_BATCH_AGENT = int(TRAIN_DATASET_LENGTH*(5/100))\n",
    "TRAINING_EPOCHS_BATCH_AGENT = int(TRAIN_DATASET_LENGTH*(5/100))\n",
    "WARM_START_EPISODES_BATCH_AGENT = 50\n",
    "TRAINING_EPOCHS_BATCH_AGENT = 50\n",
    "TRAINING_EPISODES_PER_EPOCH_BATCH_AGENT = 10\n",
    "NN_UPDATES_PER_EPOCHS_BATCH_AGENT = 1\n",
    "\n",
    "print(\"Warm-start episodes: {}.\".format(WARM_START_EPISODES_BATCH_AGENT))\n",
    "print(\"Training epochs: {}.\".format(TRAINING_EPOCHS_BATCH_AGENT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd() # Find the current directory.\n",
    "\n",
    "# Delete following directories if they exist.\n",
    "shutil.rmtree(cwd+'/__pycache__', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/wandb', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/batch_agent', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/libact', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/AL_results', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/checkpoints', ignore_errors=True)\n",
    "shutil.rmtree(cwd+'/summaries', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = DatasetCIFAR10(number_of_state_data=NUMBER_OF_STATE_DATA, train_dataset_length=TRAIN_DATASET_LENGTH)\n",
    "print(\"Train data are {}.\".format(len(dataset.train_data)))\n",
    "print(\"State data are {}.\".format(len(dataset.state_data)))\n",
    "print(\"Test data are {}.\".format(len(dataset.test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "# Deep CNN.\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# Parameters.\n",
    "input_shape = (32, 32, 3)\n",
    "optimizer = SGD(lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Create the classifier.\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(64, (3, 3), input_shape=input_shape, padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "classifier.add(Conv2D(64, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(Conv2D(128, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "classifier.add(Conv2D(128, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(Conv2D(256, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "classifier.add(Conv2D(256, (3, 3), padding='same', activation='relu', kernel_constraint=maxnorm(3)))\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(1024, activation='relu', kernel_constraint=maxnorm(3)))\n",
    "classifier.add(Dropout(0.5))\n",
    "\n",
    "classifier.add(Dense(CLASSIFIER_NUMBER_OF_CLASSES, activation='softmax'))\n",
    "\n",
    "# Compile classifier.\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "predictions = classifier.predict(dataset.train_data)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(dataset.train_labels_one_hot_encoding, axis=1)\n",
    "TARGE_PRECISION = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "print(\"Precision after training with all the data: {}.\".format(TARGE_PRECISION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the BatchAgent environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_env = LalEnvFirstAccuracy(dataset, classifier, epochs=CLASSIFIER_NUMBER_OF_EPOCHS, classifier_batch_size=CLASSIFIER_BATCH_SIZE, target_precision=TARGE_PRECISION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialise the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size=REPLAY_BUFFER_SIZE, prior_exp=PRIOROTIZED_REPLAY_EXPONENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Warm-start Episodes. | BatchAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keep track of episode duration to compute average.\n",
    "episode_durations = []\n",
    "episode_scores = []\n",
    "episode_number = 1\n",
    "episode_losses = []\n",
    "episode_precisions = []\n",
    "batches = []\n",
    "first_batch = True\n",
    "\n",
    "for _ in range(WARM_START_EPISODES_BATCH_AGENT):\n",
    "\n",
    "    print(\"Episode {}.\".format(episode_number))\n",
    "    # Reset the environment to start a new episode.\n",
    "    # The state value contains a vector representation of state of the environment (depends on the classifier).\n",
    "    # The next_action contains a vector representations of all actions available to be taken at the next step.\n",
    "    state, next_action, indicies_unknown, reward = batch_env.reset(isBatchAgent=False, target_budget=1.0)\n",
    "    done = False\n",
    "    episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "\n",
    "    # Before we reach a terminal state, make steps.\n",
    "    while not done:\n",
    "\n",
    "        # Choose a random action.\n",
    "        batch = random.choice(next_action)[0]\n",
    "        if first_batch:\n",
    "            batches.append(batch)\n",
    "            first_batch = False\n",
    "        else:\n",
    "            iteration = 1\n",
    "            while batch in batches:\n",
    "                batch = random.choice(next_action)[0]\n",
    "                iteration += 1\n",
    "                if iteration > 50:\n",
    "                    break  \n",
    "            batches.append(batch)\n",
    "\n",
    "        # Getting numbers from 0 to n_actions.\n",
    "        inputNumbers =range(0,batch_env.n_actions)\n",
    "\n",
    "        # Non-repeating using sample() function.\n",
    "        batch_actions_indices = np.array(random.sample(inputNumbers, batch))\n",
    "        action = batch\n",
    "        next_state, next_action, indicies_unknown, reward, done = batch_env.step(batch_actions_indices)\n",
    "        \n",
    "        if next_action==[]:\n",
    "            next_action.append(np.array([0]))\n",
    "\n",
    "        # Store the transition in the replay buffer.\n",
    "        replay_buffer.store_transition(state, action, reward, next_state, next_action, done)\n",
    "\n",
    "        # Get ready for the next step.\n",
    "        state = next_state\n",
    "        episode_duration += batch\n",
    "\n",
    "    episode_final_acc = batch_env.return_episode_qualities()     \n",
    "    episode_scores.append(episode_final_acc[-1])\n",
    "    episode_final_precision = batch_env.return_episode_precisions()     \n",
    "    episode_precisions.append(episode_final_precision[-1])    \n",
    "    episode_durations.append(episode_duration)  \n",
    "    episode_number+=1\n",
    "\n",
    "# Compute the average episode duration of episodes generated during the warm start procedure.\n",
    "av_episode_duration = np.mean(episode_durations)\n",
    "BIAS_INITIALIZATION = -av_episode_duration/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot total budget size per episode.\n",
    "xpoints = np.array(range(0,len(episode_durations)))\n",
    "ypoints = np.array(episode_durations)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(xpoints, ypoints, 'o', color='m')  # Plot points as blue circles.\n",
    "xnew = np.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = make_interp_spline(xpoints, ypoints, k=3)\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='m')\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot total budget size (percentage of the UD) per episode.\n",
    "xpoints = np.array(range(0,len(episode_durations)))\n",
    "ypoints = np.array([x/len(dataset.train_data) for x in episode_durations])\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(xpoints, ypoints, 'o', color='k')  # Plot points as blue circles.\n",
    "xnew = np.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = make_interp_spline(xpoints, ypoints, k=3)\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='k')\n",
    "plot_label = \"Budget per episode. *Size of unlabeled data: \" + str(len(dataset.train_data))\n",
    "plt.title(plot_label, loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Budget size (percentage of the UD)\")\n",
    "\n",
    "# Plot final achieved accuracy per episode.\n",
    "xpoints = np.array(range(0,len(episode_scores)))\n",
    "ypoints = np.array(episode_scores)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(xpoints, ypoints, 'o', color='c')  # Plot points as blue circles.\n",
    "xnew = np.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = make_interp_spline(xpoints, ypoints, k=3)\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='c')\n",
    "plt.title(\"Final achieved accuracy per episode\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"ACC\")\n",
    "legend_label = \"Maximum ACC: \" + str(max(episode_scores))[:4]\n",
    "plt.legend([legend_label])\n",
    "\n",
    "# Plot final achieved precision per episode.\n",
    "xpoints = np.array(range(0,len(episode_precisions)))\n",
    "ypoints = np.array(episode_precisions)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(xpoints, ypoints, 'o', color='y')  # Plot points as blue circles.\n",
    "xnew = np.linspace(xpoints.min(), xpoints.max(), 500)\n",
    "spl = make_interp_spline(xpoints, ypoints, k=3)\n",
    "power_smooth = spl(xnew)\n",
    "plt.plot(xnew, power_smooth, color='y')\n",
    "plt.title(\"Final achieved precision per episode\", loc = \"left\")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Precision\")\n",
    "legend_label = \"Maximum precision: \" + str(max(episode_precisions))[:4]\n",
    "plt.legend([legend_label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_start_batches = []\n",
    "i=0\n",
    "for precision in episode_precisions:\n",
    "    if precision >= max(episode_precisions):\n",
    "        warm_start_batches.append(episode_durations[i])\n",
    "    i+=1\n",
    "TARGET_BUDGET = min(warm_start_batches)/(len(dataset.train_data))\n",
    "print(\"Target budget is {}.\".format(TARGET_BUDGET))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "# Train BatchAgent with all the data.\n",
    "# But keep the ReplayBuffer the same.\n",
    "\n",
    "TRAIN_DATASET_LENGTH = 50000\n",
    "\n",
    "dataset = DatasetCIFAR10(number_of_state_data=NUMBER_OF_STATE_DATA, train_dataset_length=TRAIN_DATASET_LENGTH)\n",
    "print(\"Train data are {}.\".format(len(dataset.train_data)))\n",
    "print(\"State data are {}.\".format(len(dataset.state_data)))\n",
    "print(\"Test data are {}.\".format(len(dataset.test_data)))\n",
    "\n",
    "\"\"\"\n",
    "predictions = classifier.predict(dataset.train_data)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(dataset.train_labels_one_hot_encoding, axis=1)\n",
    "TARGE_PRECISION = precision_score(true_labels, predicted_labels, average='weighted', zero_division=0)\n",
    "print(\"Precision after training with all the data: {}.\".format(TARGE_PRECISION))\n",
    "\"\"\"\n",
    "\n",
    "batch_env = LalEnvFirstAccuracy(dataset, classifier, epochs=CLASSIFIER_NUMBER_OF_EPOCHS, classifier_batch_size=CLASSIFIER_BATCH_SIZE, target_precision=TARGE_PRECISION)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the DQN for the BatchAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_agent = DQN(experiment_dir=DIRNAME,\n",
    "            observation_length=NUMBER_OF_STATE_DATA,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            target_copy_factor=TARGET_COPY_FACTOR,\n",
    "            bias_average=BIAS_INITIALIZATION,\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do updates of the network based on the warm-start episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for update in range(NN_UPDATES_PER_EPOCHS_BATCH_AGENT):\n",
    "    print(\"Update:\", update+1)\n",
    "    minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "    td_error = batch_agent.train(minibatch)\n",
    "    replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train BatchAgent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training episodes.\n",
    "\n",
    "agent_epoch_durations = []\n",
    "agent_epoch_scores = []\n",
    "agent_epoch_precisions = []\n",
    "\n",
    "for epoch in range(TRAINING_EPOCHS_BATCH_AGENT):\n",
    "\n",
    "    print(\"Training epoch {}.\".format(epoch+1))\n",
    "    \n",
    "    # Simulate training episodes.\n",
    "    \n",
    "    agent_episode_durations = []\n",
    "    agent_episode_scores = []\n",
    "    agent_episode_precisions = []\n",
    "\n",
    "    for training_episode in range(TRAINING_EPISODES_PER_EPOCH_BATCH_AGENT):\n",
    "\n",
    "        # print(\"Episode {}.\".format(training_episode+1))\n",
    "        \n",
    "        # Reset the environment to start a new episode.\n",
    "        state, action_batch, action_unlabeled_data, reward = batch_env.reset(isBatchAgent=True, target_budget=TARGET_BUDGET)\n",
    "        done = False\n",
    "        episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        first_batch = True\n",
    "        \n",
    "        # Run an episode.\n",
    "        while not done:\n",
    "\n",
    "            if first_batch:\n",
    "                next_batch = action_batch\n",
    "                next_unlabeled_data = action_unlabeled_data\n",
    "                first_batch = False\n",
    "            else:\n",
    "                next_batch = next_action_batch_size\n",
    "                next_unlabeled_data = next_action_unlabeled_data\n",
    "\n",
    "            selected_batch, selected_indices = batch_agent.get_action(dataset=dataset, model=classifier, state=state, next_action_batch=next_batch, next_action_unlabeled_data=next_unlabeled_data)\n",
    "            next_state, next_action_batch_size, next_action_unlabeled_data, reward, done = batch_env.step(selected_indices)\n",
    "            if next_action_batch_size==[]:\n",
    "                next_action_batch_size.append(np.array([0]))\n",
    "\n",
    "            replay_buffer.store_transition(state, selected_batch, reward, next_state, next_action_batch_size, done)\n",
    "            \n",
    "            # Change the state of the environment.\n",
    "            state = next_state\n",
    "            episode_duration += selected_batch\n",
    "            print(\"Selected batch is {}.\".format(selected_batch))\n",
    "\n",
    "        agent_episode_final_acc = batch_env.return_episode_qualities()\n",
    "        agent_episode_scores.append(agent_episode_final_acc[-1])\n",
    "        agent_episode_final_precision = batch_env.return_episode_precisions()\n",
    "        agent_episode_precisions.append(agent_episode_final_precision[-1])\n",
    "        agent_episode_durations.append(episode_duration)\n",
    "        #print(\"---- Episode durations\", agent_episode_durations)\n",
    "        #print(\"\\n\")\n",
    "        \n",
    "    maximum_epoch_precision = max(agent_episode_precisions)\n",
    "    minimum_batches_for_the_maximum_epoch_precision = []\n",
    "    accuracy_for_the_maximum_epoch_precision = []\n",
    "    for i in range(len(agent_episode_precisions)):\n",
    "        if agent_episode_precisions[i] == maximum_epoch_precision:\n",
    "            minimum_batches_for_the_maximum_epoch_precision.append(agent_episode_durations[i])\n",
    "            accuracy_for_the_maximum_epoch_precision.append(agent_episode_scores[i])\n",
    "    agent_epoch_precisions.append(maximum_epoch_precision)\n",
    "    agent_epoch_scores.append(accuracy_for_the_maximum_epoch_precision)\n",
    "    agent_epoch_durations.append(min(minimum_batches_for_the_maximum_epoch_precision))\n",
    "\n",
    "    # NEURAL NETWORK UPDATES.\n",
    "    for update in range(NN_UPDATES_PER_EPOCHS_BATCH_AGENT):\n",
    "        # print(\"Update {}.\".format(update+1))\n",
    "        minibatch = replay_buffer.sample_minibatch(BATCH_SIZE)\n",
    "        td_error = batch_agent.train(minibatch)\n",
    "        replay_buffer.update_td_errors(td_error, minibatch.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End time.\n",
    "seconds = time.time() - start_time\n",
    "print(\"Total run time is {}.\".format(time.strftime(\"%H:%M:%S\",time.gmtime(seconds))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precisions.\n",
    "\n",
    "warm_start_xpoints = np.array(range(0,len(episode_precisions)))\n",
    "warm_start_ypoints = np.array([x*100 for x in episode_precisions])\n",
    "warm_start_xnew = np.linspace(warm_start_xpoints.min(), warm_start_xpoints.max(), 150)\n",
    "warm_start_spl = make_interp_spline(warm_start_xpoints, warm_start_ypoints, k=3)\n",
    "warm_start_power_smooth = warm_start_spl(warm_start_xnew)\n",
    "\n",
    "batch_agent_xpoints = np.array(range(0,len(agent_epoch_precisions)))\n",
    "batch_agent_ypoints = np.array([x*100 for x in agent_epoch_precisions])\n",
    "batch_agent_xnew = np.linspace(batch_agent_xpoints.min(), batch_agent_xpoints.max(), 150)\n",
    "batch_agent_spl = make_interp_spline(batch_agent_xpoints, batch_agent_ypoints, k=3)\n",
    "batch_agent_power_smooth = batch_agent_spl(batch_agent_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(warm_start_xnew, warm_start_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(batch_agent_xnew, batch_agent_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"Warm-start\", \"Agent\"]) \n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Precision\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot budgets.\n",
    "warm_start_xpoints = np.array(range(0,len(episode_durations)))\n",
    "warm_start_ypoints = np.array([(x/len(dataset.train_data))*100 for x in episode_durations])\n",
    "warm_start_xnew = np.linspace(warm_start_xpoints.min(), warm_start_xpoints.max(), 150)\n",
    "warm_start_spl = make_interp_spline(warm_start_xpoints, warm_start_ypoints, k=3)\n",
    "warm_start_power_smooth = warm_start_spl(warm_start_xnew)\n",
    "\n",
    "batch_agent_xpoints = np.array(range(0,len(agent_epoch_durations)))\n",
    "batch_agent_ypoints = np.array([(x/len(dataset.train_data))*100 for x in agent_epoch_durations])\n",
    "batch_agent_xnew = np.linspace(batch_agent_xpoints.min(), batch_agent_xpoints.max(), 150)\n",
    "batch_agent_spl = make_interp_spline(batch_agent_xpoints, batch_agent_ypoints, k=3)\n",
    "batch_agent_power_smooth = batch_agent_spl(batch_agent_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(warm_start_xnew, warm_start_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(batch_agent_xnew, batch_agent_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"Warm-start\", \"Agent\"]) \n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Budget\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate testing episodes.\n",
    "\n",
    "TEST_EPISODES = 50\n",
    "\n",
    "Reinforced_Active_Learning = True\n",
    "RAL_episode_durations = []\n",
    "RAL_episode_scores = []\n",
    "RAL_episode_precisions = []\n",
    "\n",
    "Random_Sampling = True\n",
    "Random_Sampling_episode_durations = []\n",
    "Random_Sampling_episode_scores = []\n",
    "Random_Sampling_episode_precisions = []\n",
    "\n",
    "for episode in range(TEST_EPISODES):\n",
    "\n",
    "    print(\"Testing episode {}.\".format(episode+1))\n",
    "    \n",
    "\n",
    "    # print(\"Episode {}.\".format(training_episode+1))\n",
    "        \n",
    "    # Reset the environment to start a new episode.\n",
    "    state, action_batch, action_unlabeled_data, reward = batch_env.reset(isBatchAgent=True, target_budget=TARGET_BUDGET)\n",
    "    random_sampling_episode_durarion = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        \n",
    "    # Run an episode.\n",
    "\n",
    "    if Reinforced_Active_Learning:\n",
    "\n",
    "        print(\"- Reinforced Active Learning.\")\n",
    "\n",
    "        RAL_episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        RAL_env = copy.deepcopy(batch_env)\n",
    "        first_batch = True\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            if first_batch:\n",
    "                next_batch = action_batch\n",
    "                next_unlabeled_data = action_unlabeled_data\n",
    "                first_batch = False\n",
    "            else:\n",
    "                next_batch = next_action_batch_size\n",
    "                next_unlabeled_data = next_action_unlabeled_data\n",
    "\n",
    "            selected_batch, selected_indices = batch_agent.get_action(dataset=dataset, model=classifier, state=state, next_action_batch=next_batch, next_action_unlabeled_data=next_unlabeled_data)\n",
    "            next_state, next_action_batch_size, next_action_unlabeled_data, reward, done = RAL_env.step(selected_indices)\n",
    "\n",
    "            RAL_episode_duration += selected_batch\n",
    "\n",
    "        agent_episode_final_acc = RAL_env.return_episode_qualities()\n",
    "        RAL_episode_scores.append(agent_episode_final_acc[-1])\n",
    "        agent_episode_final_precision = RAL_env.return_episode_precisions()\n",
    "        RAL_episode_precisions.append(agent_episode_final_precision[-1])\n",
    "        RAL_episode_durations.append(RAL_episode_duration)\n",
    "\n",
    "        # wandb.log({\"RAL | Precision\": RAL_episode_precisions[-1], \"RAL | Budget\": (RAL_episode_durations[-1]/len(dataset.train_data))*100})\n",
    "\n",
    "    if Random_Sampling:\n",
    "\n",
    "        print(\"- Random Sampling.\")\n",
    "\n",
    "        Random_Sampling_episode_duration = CLASSIFIER_NUMBER_OF_CLASSES\n",
    "        Random_Sampling_env = copy.deepcopy(batch_env)\n",
    "        Random_Sampling_env_state = copy.deepcopy(state)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "\n",
    "            batch = random.randint(1, Random_Sampling_env.n_actions)\n",
    "            # print(\"--- Random_Sampling_env.n_actions\", Random_Sampling_env.n_actions)\n",
    "            inputNumbers =range(0,Random_Sampling_env.n_actions)\n",
    "            # print(\"--- inputNumbers\", inputNumbers)\n",
    "            # print(\"--- batch\", batch)\n",
    "            batch_actions_indices = np.array(random.sample(inputNumbers, batch))\n",
    "            action = batch\n",
    "            _, next_action, _, _, done = Random_Sampling_env.step(batch_actions_indices)\n",
    "\n",
    "            Random_Sampling_episode_duration += batch\n",
    "\n",
    "        episode_final_acc = Random_Sampling_env.return_episode_qualities()     \n",
    "        Random_Sampling_episode_scores.append(episode_final_acc[-1])\n",
    "        episode_final_precision = Random_Sampling_env.return_episode_precisions()     \n",
    "        Random_Sampling_episode_precisions.append(episode_final_precision[-1])    \n",
    "        Random_Sampling_episode_durations.append(Random_Sampling_episode_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precisions.\n",
    "\n",
    "random_sampling_xpoints = np.array(range(0,len(Random_Sampling_episode_precisions)))\n",
    "random_sampling_ypoints = np.array([x*100 for x in Random_Sampling_episode_precisions])\n",
    "random_sampling_xnew = np.linspace(random_sampling_xpoints.min(), random_sampling_xpoints.max(), 150)\n",
    "random_sampling_spl = make_interp_spline(random_sampling_xpoints, random_sampling_ypoints, k=3)\n",
    "random_sampling_power_smooth = random_sampling_spl(random_sampling_xnew)\n",
    "\n",
    "reinforced_active_learning_xpoints = np.array(range(0,len(RAL_episode_precisions)))\n",
    "reinforced_active_learning_ypoints = np.array([x*100 for x in RAL_episode_precisions])\n",
    "reinforced_active_learning_xnew = np.linspace(reinforced_active_learning_xpoints.min(), reinforced_active_learning_xpoints.max(), 150)\n",
    "reinforced_active_learning_spl = make_interp_spline(reinforced_active_learning_xpoints, reinforced_active_learning_ypoints, k=3)\n",
    "reinforced_active_learning_power_smooth = reinforced_active_learning_spl(reinforced_active_learning_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(random_sampling_xnew, random_sampling_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(reinforced_active_learning_xnew, reinforced_active_learning_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"RS\", \"RAL\"]) \n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Precision\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot budgets.\n",
    "\n",
    "random_sampling_xpoints = np.array(range(0,len(Random_Sampling_episode_durations)))\n",
    "random_sampling_ypoints = np.array([(x/len(dataset.train_data))*100 for x in Random_Sampling_episode_durations])\n",
    "random_sampling_xnew = np.linspace(random_sampling_xpoints.min(), random_sampling_xpoints.max(), 150)\n",
    "random_sampling_spl = make_interp_spline(random_sampling_xpoints, random_sampling_ypoints, k=3)\n",
    "random_sampling_power_smooth = random_sampling_spl(random_sampling_xnew)\n",
    "\n",
    "reinforced_active_learning_xpoints = np.array(range(0,len(RAL_episode_durations)))\n",
    "reinforced_active_learning_ypoints = np.array([(x/len(dataset.train_data))*100 for x in RAL_episode_durations])\n",
    "reinforced_active_learning_xnew = np.linspace(reinforced_active_learning_xpoints.min(), reinforced_active_learning_xpoints.max(), 150)\n",
    "reinforced_active_learning_spl = make_interp_spline(reinforced_active_learning_xpoints, reinforced_active_learning_ypoints, k=3)\n",
    "reinforced_active_learning_power_smooth = reinforced_active_learning_spl(reinforced_active_learning_xnew)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot(random_sampling_xnew, random_sampling_power_smooth, color='y', linewidth=2.5)\n",
    "plt.plot(reinforced_active_learning_xnew, reinforced_active_learning_power_smooth, color='m', linewidth=2.5)\n",
    "plt.legend([\"RS\", \"RAL\"]) \n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Budget\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
